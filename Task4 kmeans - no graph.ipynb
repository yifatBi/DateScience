{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Task4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\Yuval\\Desktop\\DateScience-master\\task4.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_1=-1.0, _2=-1.0, _3=-1.0, _4=-1.0, _5=-1.0, _6=-1.0, _7=-1.0, _8=-1.0, _9=-1.0, _10=-1.0, _11=-1.0, _12=-1.0, _13=-1.0, _14=-1.0, _15=-1.0, _16=-1.0, _17=-1.0, _18=-1.0, _19=-1.0, _20=-1.0, _21=-1.0, _22=9.47176684881603, _23=10.018214936247723, _24=0.0, _25=-1.0, _26=-1.0, _27=-1.0, _28=-1.0, _29=14.025500910746812, _30=14.025500910746812, _31=14.025500910746812, _32=14.025500910746812, _33=-1.0, _34=-1.0, _35=-1.0, _36=-1.0, _37=-1.0, _38=-1.0, _39=-1.0, _40=-1.0, _41=-1.0, _42=-1.0, _43=-1.0, _44=-1.0, _45=-1.0, _46=-1.0, _47=-1.0, _48=-1.0, _49=24.043715846994534, _50=32.05828779599271, _51=-1.0, _52=-1.0, _53=-1.0, _54=-1.0, _55=-1.0, _56=-1.0, _57=-1.0, _58=51.7304189435337, _59=61.56648451730419, _60=71.5846994535519, _61=-1.0, _62=-1.0, _63=53.916211293260474, _64=-1.0, _65=71.5846994535519, _66=-1.0, _67=32.05828779599271, _68=-1.0, _69=-1.0, _70=-1.0, _71=-1.0, _72=-1.0, _73=-1.0, _74=-1.0, _75=45.90163934426229, _76=-1.0, _77=34.06193078324226, _78=-1.0, _79=47.540983606557376, _80=-1.0, _81=-1.0, _82=-1.0, _83=43.169398907103826, _84=-1.0, _85=-1.0, _86=42.25865209471767, _87=-1.0, _88=-1.0, _89=100.0, _90=-1.0, _91=-1.0, _92=-1.0, _93=-1.0, _94=-1.0, _95=-1.0, _96=-1.0, _97=82.87795992714025, _98=-1.0, _99=-1.0, _100=-1.0, _101=21.493624772313296, _102=-1.0, _103=-1.0, _104=-1.0, _105=59.92714025500911, _106=-1.0, _107=-1.0, _108=-1.0, _109=-1.0, _110=-1.0, _111=-1.0, _112=-1.0, _113=-1.0, _114=-1.0, _115=-1.0, _116=-1.0, _117=-1.0, _118=-1.0, _119=-1.0, _120=-1.0, _121=-1.0, _122=-1.0, _123=-1.0, _124=-1.0, _125=-1.0, _126=-1.0, _127=-1.0, _128=-1.0, _129=-1.0, _130=-1.0, _131=-1.0, _132=-1.0, _133=-1.0, _134=-1.0, _135=-1.0, _136=-1.0, _137=-1.0, _138=-1.0, _139=-1.0, _140=-1.0, _141=-1.0, _142=-1.0, _143=-1.0, _144=-1.0, _145=-1.0, _146=-1.0, _147=-1.0, _148=-1.0, _149=-1.0, _150=-1.0, _151=-1.0, _152=-1.0, _153=-1.0, _154=-1.0, _155=-1.0, _156=-1.0, _157=-1.0, _158=-1.0, _159=-1.0, _160=-1.0, _161='Aloft Harlem')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, _11: double, _12: double, _13: double, _14: double, _15: double, _16: double, _17: double, _18: double, _19: double, _20: double, _21: double, _22: double, _23: double, _24: double, _25: double, _26: double, _27: double, _28: double, _29: double, _30: double, _31: double, _32: double, _33: double, _34: double, _35: double, _36: double, _37: double, _38: double, _39: double, _40: double, _41: double, _42: double, _43: double, _44: double, _45: double, _46: double, _47: double, _48: double, _49: double, _50: double, _51: double, _52: double, _53: double, _54: double, _55: double, _56: double, _57: double, _58: double, _59: double, _60: double, _61: double, _62: double, _63: double, _64: double, _65: double, _66: double, _67: double, _68: double, _69: double, _70: double, _71: double, _72: double, _73: double, _74: double, _75: double, _76: double, _77: double, _78: double, _79: double, _80: double, _81: double, _82: double, _83: double, _84: double, _85: double, _86: double, _87: double, _88: double, _89: double, _90: double, _91: double, _92: double, _93: double, _94: double, _95: double, _96: double, _97: double, _98: double, _99: double, _100: double, _101: double, _102: double, _103: double, _104: double, _105: double, _106: double, _107: double, _108: double, _109: double, _110: double, _111: double, _112: double, _113: double, _114: double, _115: double, _116: double, _117: double, _118: double, _119: double, _120: double, _121: double, _122: double, _123: double, _124: double, _125: double, _126: double, _127: double, _128: double, _129: double, _130: double, _131: double, _132: double, _133: double, _134: double, _135: double, _136: double, _137: double, _138: double, _139: double, _140: double, _141: double, _142: double, _143: double, _144: double, _145: double, _146: double, _147: double, _148: double, _149: double, _150: double, _151: double, _152: double, _153: double, _154: double, _155: double, _156: double, _157: double, _158: double, _159: double, _160: double, _161: string, features: vector]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "feat_cols = ['_' + str(i) for i in range(1, 161)]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol='features')\n",
    "final_data = vec_assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scalerModel = scaler.fit(final_data)\n",
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n",
    "\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans_41728e41c022d855e343\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   10|\n",
      "|         3|   36|\n",
      "|         2|   17|\n",
      "|         0|   87|\n",
      "+----------+-----+\n",
      "\n",
      "Errors = 19305.884654414487\n"
     ]
    }
   ],
   "source": [
    "# Run Kmeans\n",
    "kmeans4 = KMeans(featuresCol='scaledFeatures',k=4)\n",
    "model_k4 = kmeans4.fit(cluster_final_data)\n",
    "wssse = model_k4.computeCost(cluster_final_data)\n",
    "transformed_data = model_k4.transform(cluster_final_data)\n",
    "predictions = transformed_data.groupBy('prediction')\n",
    "predictions.count().show()\n",
    "\n",
    "print(\"Errors = \"+ str(wssse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
