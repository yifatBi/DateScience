{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Task4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "df = spark.read.csv(r\"C:\\Users\\Yuval\\Desktop\\DateScience-master\\task4.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_1=-1.0, _2=-1.0, _3=-1.0, _4=-1.0, _5=-1.0, _6=-1.0, _7=-1.0, _8=-1.0, _9=-1.0, _10=-1.0, _11=-1.0, _12=-1.0, _13=-1.0, _14=-1.0, _15=-1.0, _16=-1.0, _17=-1.0, _18=-1.0, _19=-1.0, _20=-1.0, _21=-1.0, _22=9.47176684881603, _23=10.018214936247723, _24=0.0, _25=-1.0, _26=-1.0, _27=-1.0, _28=-1.0, _29=14.025500910746812, _30=14.025500910746812, _31=14.025500910746812, _32=14.025500910746812, _33=-1.0, _34=-1.0, _35=-1.0, _36=-1.0, _37=-1.0, _38=-1.0, _39=-1.0, _40=-1.0, _41=-1.0, _42=-1.0, _43=-1.0, _44=-1.0, _45=-1.0, _46=-1.0, _47=-1.0, _48=-1.0, _49=24.043715846994534, _50=32.05828779599271, _51=-1.0, _52=-1.0, _53=-1.0, _54=-1.0, _55=-1.0, _56=-1.0, _57=-1.0, _58=51.7304189435337, _59=61.56648451730419, _60=71.5846994535519, _61=-1.0, _62=-1.0, _63=53.916211293260474, _64=-1.0, _65=71.5846994535519, _66=-1.0, _67=32.05828779599271, _68=-1.0, _69=-1.0, _70=-1.0, _71=-1.0, _72=-1.0, _73=-1.0, _74=-1.0, _75=45.90163934426229, _76=-1.0, _77=34.06193078324226, _78=-1.0, _79=47.540983606557376, _80=-1.0, _81=-1.0, _82=-1.0, _83=43.169398907103826, _84=-1.0, _85=-1.0, _86=42.25865209471767, _87=-1.0, _88=-1.0, _89=100.0, _90=-1.0, _91=-1.0, _92=-1.0, _93=-1.0, _94=-1.0, _95=-1.0, _96=-1.0, _97=82.87795992714025, _98=-1.0, _99=-1.0, _100=-1.0, _101=21.493624772313296, _102=-1.0, _103=-1.0, _104=-1.0, _105=59.92714025500911, _106=-1.0, _107=-1.0, _108=-1.0, _109=-1.0, _110=-1.0, _111=-1.0, _112=-1.0, _113=-1.0, _114=-1.0, _115=-1.0, _116=-1.0, _117=-1.0, _118=-1.0, _119=-1.0, _120=-1.0, _121=-1.0, _122=-1.0, _123=-1.0, _124=-1.0, _125=-1.0, _126=-1.0, _127=-1.0, _128=-1.0, _129=-1.0, _130=-1.0, _131=-1.0, _132=-1.0, _133=-1.0, _134=-1.0, _135=-1.0, _136=-1.0, _137=-1.0, _138=-1.0, _139=-1.0, _140=-1.0, _141=-1.0, _142=-1.0, _143=-1.0, _144=-1.0, _145=-1.0, _146=-1.0, _147=-1.0, _148=-1.0, _149=-1.0, _150=-1.0, _151=-1.0, _152=-1.0, _153=-1.0, _154=-1.0, _155=-1.0, _156=-1.0, _157=-1.0, _158=-1.0, _159=-1.0, _160=-1.0, _161='Aloft Harlem')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "feat_cols = ['_' + str(i) for i in range(1, 161)]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol='features')\n",
    "final_data = vec_assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scalerModel = scaler.fit(final_data)\n",
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n",
    "\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=4\n",
      "Within Set Sum of Squared Errors = 19142.59055740316\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   74|\n",
      "|         3|   32|\n",
      "|         2|   17|\n",
      "|         0|   27|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Kmeans\n",
    "kmeans4 = KMeans(featuresCol='scaledFeatures',k=4)\n",
    "model_k4 = kmeans4.fit(cluster_final_data)\n",
    "result_k4 = model_k4.computeCost(cluster_final_data)\n",
    "\n",
    "print(Errors = \" + str(result_k4))\n",
    "model_k4.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
